{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.data import Tuple, Pad\n",
    "from functools import partial\n",
    "import paddle\n",
    "paddle.set_device(\"cpu\")\n",
    "from paddle.io import DataLoader\n",
    "from paddlenlp.transformers import T5Tokenizer as PDT5Tokenizer\n",
    "from transformers.models.t5 import T5Tokenizer as HGT5Tokenizer\n",
    "pd_tokenizer = PDT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "hg_tokenizer = HGT5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "train_ds,dev_ds = load_dataset(\"glue\", \"rte\", splits=[\"train\",\"dev\"])\n",
    "id2label = dict(zip(range(len(train_ds.label_list)),train_ds.label_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\yujun\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\util\\selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import namedtuple, Mapping\n",
      "C:\\Users\\yujun\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n",
      "C:\\Users\\yujun\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\_distutils_hack\\__init__.py:19: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
      "\u001b[32m[2021-08-23 16:02:59,647] [    INFO]\u001b[0m - Already cached C:\\Users\\yujun\\.paddlenlp\\models\\t5-base\\spiece.model\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    if not is_test:\n",
    "        label_text = id2label[example[\"labels\"]]\n",
    "        target = tokenizer(label_text,return_token_type_ids=False,return_attention_mask=True)\n",
    "\n",
    "    if (int(is_test) + len(example)) == 2:\n",
    "        source = tokenizer(example[\"sentence\"], max_seq_len=max_seq_length,return_attention_mask=True)\n",
    "    else:\n",
    "        source = tokenizer(\n",
    "            example[\"sentence1\"],\n",
    "            text_pair=example[\"sentence2\"],\n",
    "            max_seq_len=max_seq_length,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "    if not is_test:\n",
    "        return source[\"input_ids\"],source[\"attention_mask\"], target[\"input_ids\"], target[\"attention_mask\"]\n",
    "    else:\n",
    "        return source[\"input_ids\"],source[\"attention_mask\"]\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=pd_tokenizer,\n",
    "    max_seq_length=128,\n",
    "    is_test=False\n",
    ")\n",
    "train_ds = train_ds.map(trans_func, lazy=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=pd_tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=pd_tokenizer.pad_token_id),  # attention_mask\n",
    "    Pad(axis=0, pad_val=-100),  # lm_labels\n",
    "    Pad(axis=0, pad_val=pd_tokenizer.pad_token_id), # decoder_attention_mask\n",
    "): fn(samples)\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    collate_fn=batchify_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    return_list=True,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for e in train_data_loader:\n",
    "    print(e)\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Tensor(shape=[2, 58], dtype=int32, place=CPUPlace, stop_gradient=True,\n",
      "       [[438  , 2386 , 12037, 53   , 34   , 429  , 1835 , 35   , 165  , 3429 , 3    , 8389 , 12   , 1733 , 8    , 3654 , 4029 , 6    , 420  , 837  , 14261, 32   , 17   , 23   , 1016 , 6    , 14702, 3588 , 6    , 243  , 8    , 192  , 4458 , 906  , 12   , 1132 , 8    , 1419 , 5    , 1    , 14702, 3588 , 19   , 3    , 9    , 5237 , 13   , 3    , 9    , 420  , 837  , 14261, 32   , 17   , 23   , 1016 , 5    , 1    ],\n",
      "        [16908, 15507, 348  , 6    , 3    , 9    , 973  , 5812 , 44   , 5847 , 31   , 7    , 14914, 1015 , 636  , 6    , 65   , 3    , 8689 , 16   , 2405 , 3535 , 973  , 11   , 1284 , 973  , 21   , 72   , 145  , 460  , 203  , 5    , 1    , 16908, 15507, 348  , 19   , 3    , 9    , 973  , 5812 , 5    , 1    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    , 0    ]]), Tensor(shape=[2, 58], dtype=int32, place=CPUPlace, stop_gradient=True,\n",
      "       [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), Tensor(shape=[2, 6], dtype=int32, place=CPUPlace, stop_gradient=True,\n",
      "       [[ 59 ,  834,  35 , 5756,  297,  1  ],\n",
      "        [ 3  ,  35 , 5756,  297,  1  , -100]]), Tensor(shape=[2, 6], dtype=int32, place=CPUPlace, stop_gradient=True,\n",
      "       [[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from transformers.models.t5 import T5ForConditionalGeneration as PTT5ForConditionalGeneration\n",
    "PREFIX = \"E:/paddle论文复现/suoyoudaima/paddle_t5-old/\"\n",
    "pt_model = PTT5ForConditionalGeneration.from_pretrained(PREFIX+\"google/t5-small\")\n",
    "pt_model.eval()\n",
    "print(\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import torch\n",
    "e = iter(train_data_loader).next()\n",
    "x0 = torch.tensor(e[0].numpy())\n",
    "x1 = torch.tensor(e[1].numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from paddlenlp.transformers import T5ForConditionalGeneration as PDT5ForConditionalGeneration\n",
    "pd_model = PDT5ForConditionalGeneration.from_pretrained(PREFIX+\"paddle/t5-small\")\n",
    "pd_model.eval()\n",
    "print(\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def compare(a, b):\n",
    "    a = torch.tensor(a.numpy()).float()\n",
    "    b = torch.tensor(b.numpy()).float()\n",
    "    meandif = (a - b).abs().mean()\n",
    "    maxdif = (a - b).abs().max()\n",
    "    print(\"mean difference:\", meandif)\n",
    "    print(\"max difference:\", maxdif)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "pd_outs = pd_model.generate(input_ids=e[0],attention_mask=e[1],num_beams=4,min_length=4,max_length=50-1,decode_strategy=\"beam_search\",early_stopping=True,length_penalty=2.0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "pt_outs = pt_model.generate(input_ids=x0,attention_mask=x1,num_beams=4,min_length=4,max_length=50,early_stopping=True,length_penalty=2.0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is_beam_gen_modeis_beam_gen_modeis_beam_gen_modeis_beam_gen_mode\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "compare(pt_outs,pd_outs[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean difference: tensor(0.)\n",
      "max difference: tensor(0.)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "086317466957d500e1e3add5d1080e4cde135e955220d9fc98fd7fe59df8a909"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}